{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fca6b5-218c-4a74-945a-cc795a12c4a5",
   "metadata": {},
   "source": [
    "# Vision Transformer(ViT)模型原理及PyTorch逐行实现\n",
    "\n",
    "来自b站up主deep_thoughts 合集【PyTorch源码教程与前沿人工智能算法复现讲解】\n",
    "\n",
    "P_28_Vision Transformer(ViT)模型原理及PyTorch逐行实现：\n",
    "\n",
    "https://www.bilibili.com/video/BV1cS4y1M7wo/?spm_id_from=pageDriver&vd_source=18e91d849da09d846f771c89a366ed40\n",
    "\n",
    "***论文***\n",
    "\n",
    "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale：\n",
    "\n",
    "https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d59e36-50f9-4ca1-ad67-a4686ccd7f3d",
   "metadata": {},
   "source": [
    " ## Transformer 模型特点\n",
    " \n",
    " * 无先验假设 （例如：局部关联性、有序建模性）\n",
    " * 核心计算在于自注意力机制，平方复杂度\n",
    " * 数据量的要求与归纳偏置的引入成反比\n",
    " \n",
    " 归纳偏置：人类通过归纳法所总结的经验，把经验带入到设计模型的过程之中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505782de-93df-46f0-af1a-58c75a3be94c",
   "metadata": {},
   "source": [
    "## Transformer 使用类型\n",
    "\n",
    "* Encoder only： BERT、分类任务、非流式任务、**Vision Transformer（ViT）**\n",
    "* Decoder only： GPT系列、语言建模、自回归生成任务、流式任务\n",
    "* Encoder-Decoder： 机器翻译、语音识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8abf3-6979-4bef-90ce-2027b7b04919",
   "metadata": {},
   "source": [
    "## Vision Transformer（ViT）\n",
    "\n",
    "* DNN perspective\n",
    "  * image2patch\n",
    "  * patch2embedding\n",
    "* CNN perspective\n",
    "  * 2D convolution over image\n",
    "  * flatten the output feature map\n",
    "* class token embedding\n",
    "* position embedding\n",
    "  * interpolation when inference\n",
    "* Transformer Encoder\n",
    "* classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca5f494-fc75-45a9-a6b6-2529555a619c",
   "metadata": {},
   "source": [
    "## 代码实现\n",
    "\n",
    "***相关资料***\n",
    "\n",
    "torch.nn.Unfold 官方文档：https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html\n",
    "\n",
    "torch.tile 官方文档：https://pytorch.org/docs/stable/generated/torch.tile.html\n",
    "\n",
    "torch.nn.TransformerEncoder 官方文档：https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2467325-5c04-4ba9-a5e1-a9352aef9de5",
   "metadata": {},
   "source": [
    "## step1 convert image to embedding vector sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283df65f-3795-4d34-90ed-3fc537c5b13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -7.2940,  11.0761,  -8.8822,  -5.9179,  -1.0398,   0.8863,  -9.5755,\n",
      "            8.2882],\n",
      "         [ -4.0738,   9.6549,  -5.3832,  -4.6684, -16.4855,  -2.5323, -17.7172,\n",
      "            0.0493],\n",
      "         [ -7.7384,   7.3792,   4.9279,   2.4082,   2.2541,   5.4289,   4.0024,\n",
      "           -7.1342],\n",
      "         [  3.4040,  -0.5489,  11.0364,  -8.9861,   8.5651,   3.1418,   3.6907,\n",
      "            5.6323]]])\n",
      "tensor([[[ -7.2940,  11.0760,  -8.8822,  -5.9179,  -1.0398,   0.8863,  -9.5755,\n",
      "            8.2882],\n",
      "         [ -4.0738,   9.6549,  -5.3832,  -4.6684, -16.4855,  -2.5323, -17.7172,\n",
      "            0.0493],\n",
      "         [ -7.7384,   7.3792,   4.9279,   2.4082,   2.2541,   5.4289,   4.0024,\n",
      "           -7.1342],\n",
      "         [  3.4040,  -0.5489,  11.0364,  -8.9861,   8.5651,   3.1418,   3.6907,\n",
      "            5.6323]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def image2emb_naive(image, patch_size, weight):\n",
    "    # image shape: bs*channel*h*w\n",
    "    patch = F.unfold(image, kernel_size=patch_size, stride=patch_size).transpose(-1, -2)\n",
    "    patch_embedding = patch @ weight\n",
    "    return patch_embedding\n",
    "    \n",
    "def image2emb_conv(image, kernel, stride):\n",
    "    conv_output = F.conv2d(image, kernel, stride=stride)  # bs*oc*oh*ow\n",
    "    bs, oc, oh, ow = conv_output.shape\n",
    "    patch_embedding = conv_output.reshape((bs, oc, oh*ow)).transpose(-1, -2)\n",
    "    return patch_embedding\n",
    "\n",
    "# test code for image2emb\n",
    "bs, ic, image_h, image_w = 1, 3, 8, 8\n",
    "patch_size = 4\n",
    "model_dim = 8\n",
    "patch_depth = patch_size*patch_size*ic\n",
    "image = torch.randn(bs, ic, image_h, image_w)\n",
    "weight = torch.randn(patch_depth, model_dim)  # model_dim是输出通道数目，patch_depth是卷积核的面积乘以输入通道数\n",
    "\n",
    "patch_embedding_naive = image2emb_naive(image, patch_size, weight)  # 分块方法得到embedding\n",
    "kernel = weight.transpose(0, 1).reshape((-1, ic, patch_size, patch_size))  # oc*ic*kh*kw\n",
    "\n",
    "patch_embedding_conv = image2emb_conv(image, kernel, patch_size)  # 二维卷积方法得到embedding\n",
    "print(patch_embedding_naive)\n",
    "print(patch_embedding_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b7f7c2-78ec-4af3-ad32-ab9c10698343",
   "metadata": {},
   "source": [
    "## step2 prepare CLS token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41c7cc6-63e9-4c59-a526-7952cd5e4696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "cls_token_embedding = torch.randn(bs, 1, model_dim, requires_grad=True)\n",
    "token_embedding = torch.cat([cls_token_embedding, patch_embedding_conv], dim=1)\n",
    "print(token_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614547e-db62-47d8-9f69-37caabfb7431",
   "metadata": {},
   "source": [
    "## step3 add position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6e9306-637a-41d6-a7df-4b685f2233b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8])\n",
      "torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "max_num_token = 16\n",
    "\n",
    "position_embedding_table = torch.randn(max_num_token, model_dim, requires_grad=True)\n",
    "seq_len = token_embedding.shape[1]\n",
    "# ...\n",
    "# 忽略mask\n",
    "# ...\n",
    "print(position_embedding_table[:seq_len].shape)\n",
    "position_embedding = torch.tile(position_embedding_table[:seq_len], [token_embedding.shape[0], 1, 1])\n",
    "print(position_embedding.shape)\n",
    "\n",
    "token_embedding += position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2c502-439f-4d07-a2a4-f50a594df30b",
   "metadata": {},
   "source": [
    "## step4 pass embedding to Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "713cda50-dc26-496d-a433-845a37b05382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "encoder_output = transformer_encoder(token_embedding)\n",
    "print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a4b8d-cc71-4c07-b1d0-1b9273af4ee8",
   "metadata": {},
   "source": [
    "## step5 do clssification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f36bf78-a63c-43eb-9b1f-26a53a04c8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6058, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "label = torch.randint(10, (bs,))\n",
    "\n",
    "cls_token_output = encoder_output[:, 0, :]\n",
    "linear_layer = nn.Linear(model_dim, num_classes)\n",
    "logits = linear_layer(cls_token_output)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe0c6a-5852-42e0-894b-8e4e828ee7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
